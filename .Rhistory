rate2 <- rep(0,10)
ifsuc2 <- rep("SUC",10)
rate3 <- rep(0,10)
ifsuc3 <- rep("SUC",10)
for (j in 1:10){
train <- data[-testIDs[,j], ]
test <- data[testIDs[,j], ]
test.y <- test[, c(1, 83)]
test <- test[, -83]
write.csv(train,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/train.csv",row.names=FALSE)
write.csv(test, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test.csv",row.names=FALSE)
write.csv(test.y, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test_y.csv",row.names=FALSE)
source('/Users/mahale/Documents/Punam/MSC/PSL/Project1/pmahale2_mymain.R')#your model here.make sure your model outputs "mysubmission.txt")
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission1.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc[j] <- "FAIL"
}
}
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission2.txt")
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate2[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc2[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc2[j] <- "FAIL"
}
}
}
result <- data.frame(rate=rate,ifsuc=ifsuc,rate2=rate2,ifsuc2=ifsuc2,rate3=rate3,issuc3=ifsuc3)
write.csv(result,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/finalresult.csv",row.names = FALSE)
# Assume the Y value for the test data is stored in a two-column
# data frame named "test.y":
# col 1: PID
# col 2: Sale_Price
#code to test if a specific model satisfies the criteria. will output a vector showing which split fails
#it's more strict than the instructor's requirement. Because the instructor requires that for each split, as long as one model success, then you success.
#since each of us take over one model separately, we can only test one model. Hope our model successes consistently
#we could modified it on sunday to make it test for three models at the same time
#my poor english, ni neng kan dong ba?
testIDs = read.table("/Users/mahale/Documents/Punam/MSC/PSL/Project1/project1_testIDs.dat")
data <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/Ames_data.csv")
firstfivecriteria <- 0.125 #criteria applied to the first five split
secondfivecriteria <- 0.135 #criteria applied to the second five splits
ifsuc <- rep("SUC",10)
rate <- rep(0,10)
rate2 <- rep(0,10)
ifsuc2 <- rep("SUC",10)
rate3 <- rep(0,10)
ifsuc3 <- rep("SUC",10)
for (j in 1:10){
train <- data[-testIDs[,j], ]
test <- data[testIDs[,j], ]
test.y <- test[, c(1, 83)]
test <- test[, -83]
write.csv(train,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/train.csv",row.names=FALSE)
write.csv(test, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test.csv",row.names=FALSE)
write.csv(test.y, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test_y.csv",row.names=FALSE)
source('/Users/mahale/Documents/Punam/MSC/PSL/Project1/pmahale2_mymain.R')#your model here.make sure your model outputs "mysubmission.txt")
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission1.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc[j] <- "FAIL"
}
}
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission2.txt")
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate2[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc2[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc2[j] <- "FAIL"
}
}
}
result <- data.frame(rate=rate,ifsuc=ifsuc,rate2=rate2,ifsuc2=ifsuc2,rate3=rate3,issuc3=ifsuc3)
write.csv(result,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/finalresult.csv",row.names = FALSE)
# Assume the Y value for the test data is stored in a two-column
# data frame named "test.y":
# col 1: PID
# col 2: Sale_Price
#code to test if a specific model satisfies the criteria. will output a vector showing which split fails
#it's more strict than the instructor's requirement. Because the instructor requires that for each split, as long as one model success, then you success.
#since each of us take over one model separately, we can only test one model. Hope our model successes consistently
#we could modified it on sunday to make it test for three models at the same time
#my poor english, ni neng kan dong ba?
testIDs = read.table("/Users/mahale/Documents/Punam/MSC/PSL/Project1/project1_testIDs.dat")
data <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/Ames_data.csv")
firstfivecriteria <- 0.125 #criteria applied to the first five split
secondfivecriteria <- 0.135 #criteria applied to the second five splits
ifsuc <- rep("SUC",10)
rate <- rep(0,10)
rate2 <- rep(0,10)
ifsuc2 <- rep("SUC",10)
rate3 <- rep(0,10)
ifsuc3 <- rep("SUC",10)
for (j in 1:10){
train <- data[-testIDs[,j], ]
test <- data[testIDs[,j], ]
test.y <- test[, c(1, 83)]
test <- test[, -83]
write.csv(train,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/train.csv",row.names=FALSE)
write.csv(test, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test.csv",row.names=FALSE)
write.csv(test.y, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test_y.csv",row.names=FALSE)
source('/Users/mahale/Documents/Punam/MSC/PSL/Project1/pmahale2_mymain.R')#your model here.make sure your model outputs "mysubmission.txt")
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission1.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc[j] <- "FAIL"
}
}
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission2.txt")
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate2[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc2[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc2[j] <- "FAIL"
}
}
}
result <- data.frame(rate=rate,ifsuc=ifsuc,rate2=rate2,ifsuc2=ifsuc2,rate3=rate3,issuc3=ifsuc3)
write.csv(result,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/finalresult.csv",row.names = FALSE)
result <- data.frame(rate=rate,ifsuc=ifsuc,rate2=rate2,ifsuc2=ifsuc2,rate3=rate3,issuc3=ifsuc3)
write.csv(result,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/finalresult.csv",row.names = FALSE)
# Assume the Y value for the test data is stored in a two-column
# data frame named "test.y":
# col 1: PID
# col 2: Sale_Price
#code to test if a specific model satisfies the criteria. will output a vector showing which split fails
#it's more strict than the instructor's requirement. Because the instructor requires that for each split, as long as one model success, then you success.
#since each of us take over one model separately, we can only test one model. Hope our model successes consistently
#we could modified it on sunday to make it test for three models at the same time
#my poor english, ni neng kan dong ba?
testIDs = read.table("/Users/mahale/Documents/Punam/MSC/PSL/Project1/project1_testIDs.dat")
data <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/Ames_data.csv")
firstfivecriteria <- 0.125 #criteria applied to the first five split
secondfivecriteria <- 0.135 #criteria applied to the second five splits
ifsuc <- rep("SUC",10)
rate <- rep(0,10)
rate2 <- rep(0,10)
ifsuc2 <- rep("SUC",10)
rate3 <- rep(0,10)
ifsuc3 <- rep("SUC",10)
for (j in 1:10){
train <- data[-testIDs[,j], ]
test <- data[testIDs[,j], ]
test.y <- test[, c(1, 83)]
test <- test[, -83]
write.csv(train,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/train.csv",row.names=FALSE)
write.csv(test, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test.csv",row.names=FALSE)
write.csv(test.y, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test_y.csv",row.names=FALSE)
source('/Users/mahale/Documents/Punam/MSC/PSL/Project1/pmahale2_mymain.R')#your model here.make sure your model outputs "mysubmission.txt")
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission1.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc[j] <- "FAIL"
}
}
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission2.txt")
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate2[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc2[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc2[j] <- "FAIL"
}
}
}
result <- data.frame(rate=rate,ifsuc=ifsuc,rate2=rate2,ifsuc2=ifsuc2,rate3=rate3,issuc3=ifsuc3)
write.csv(result,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/finalresult.csv",row.names = FALSE)
str(data)
str(train.x)
str(dataset)
str(train)
data <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/Ames_data.csv")
str(data)
data$Year_Built
max(data$Year_Built)
min(data$Year_Built)
min()
min(data$Year_Built)
?exp
# Assume the Y value for the test data is stored in a two-column
# data frame named "test.y":
# col 1: PID
# col 2: Sale_Price
#code to test if a specific model satisfies the criteria. will output a vector showing which split fails
#it's more strict than the instructor's requirement. Because the instructor requires that for each split, as long as one model success, then you success.
#since each of us take over one model separately, we can only test one model. Hope our model successes consistently
#we could modified it on sunday to make it test for three models at the same time
#my poor english, ni neng kan dong ba?
testIDs = read.table("/Users/mahale/Documents/Punam/MSC/PSL/Project1/project1_testIDs.dat")
data <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/Ames_data.csv")
firstfivecriteria <- 0.125 #criteria applied to the first five split
secondfivecriteria <- 0.135 #criteria applied to the second five splits
ifsuc <- rep("SUC",10)
rate <- rep(0,10)
rate2 <- rep(0,10)
ifsuc2 <- rep("SUC",10)
rate3 <- rep(0,10)
ifsuc3 <- rep("SUC",10)
for (j in 1:10){
train <- data[-testIDs[,j], ]
test <- data[testIDs[,j], ]
test.y <- test[, c(1, 83)]
test <- test[, -83]
write.csv(train,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/train.csv",row.names=FALSE)
write.csv(test, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test.csv",row.names=FALSE)
write.csv(test.y, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test_y.csv",row.names=FALSE)
source('/Users/mahale/Documents/Punam/MSC/PSL/Project1/pmahale2_mymain.R')#your model here.make sure your model outputs "mysubmission.txt")
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission1.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc[j] <- "FAIL"
}
}
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission2.txt")
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate2[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc2[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc2[j] <- "FAIL"
}
}
}
result <- data.frame(rate=rate,ifsuc=ifsuc,rate2=rate2,ifsuc2=ifsuc2,rate3=rate3,issuc3=ifsuc3)
write.csv(result,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/finalresult.csv",row.names = FALSE)
source('/Users/mahale/Documents/Punam/MSC/PSL/Project1/mymain.R')#your model here.make sure your model outputs "mysubmission.txt")
# Assume the Y value for the test data is stored in a two-column
# data frame named "test.y":
# col 1: PID
# col 2: Sale_Price
#code to test if a specific model satisfies the criteria. will output a vector showing which split fails
#it's more strict than the instructor's requirement. Because the instructor requires that for each split, as long as one model success, then you success.
#since each of us take over one model separately, we can only test one model. Hope our model successes consistently
#we could modified it on sunday to make it test for three models at the same time
#my poor english, ni neng kan dong ba?
testIDs = read.table("/Users/mahale/Documents/Punam/MSC/PSL/Project1/project1_testIDs.dat")
data <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/Ames_data.csv")
firstfivecriteria <- 0.125 #criteria applied to the first five split
secondfivecriteria <- 0.135 #criteria applied to the second five splits
ifsuc <- rep("SUC",10)
rate <- rep(0,10)
rate2 <- rep(0,10)
ifsuc2 <- rep("SUC",10)
rate3 <- rep(0,10)
ifsuc3 <- rep("SUC",10)
for (j in 1:10){
train <- data[-testIDs[,j], ]
test <- data[testIDs[,j], ]
test.y <- test[, c(1, 83)]
test <- test[, -83]
write.csv(train,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/train.csv",row.names=FALSE)
write.csv(test, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test.csv",row.names=FALSE)
write.csv(test.y, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test_y.csv",row.names=FALSE)
source('/Users/mahale/Documents/Punam/MSC/PSL/Project1/mymain.R')#your model here.make sure your model outputs "mysubmission.txt")
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission1.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc[j] <- "FAIL"
}
}
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission2.txt")
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate2[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc2[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc2[j] <- "FAIL"
}
}
}
result <- data.frame(rate=rate,ifsuc=ifsuc,rate2=rate2,ifsuc2=ifsuc2,rate3=rate3,issuc3=ifsuc3)
write.csv(result,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/finalresult.csv",row.names = FALSE)
t = c(0.116031166
, 0.117827294
, 0.117210743
, 0.115469608
, 0.108332952
, 0.131607387
, 0.133555658
, 0.127937096
, 0.126114222
, 0.123555592)
t > 0.5
t < 0.125
t < 0.135
a = c(0.122737307
, 0.11803662
,0.120688172
, 0.119602609
, 0.111685557
, 0.133410729
, 0.12662531
, 0.120330481
, 0.130230783
, 0.123696289
)
a < 0.135
a < 0.125
lassoPrediction
summary(lassoPrediction)
summary(cv.out)
min(a)
mint
min(t)
t
a
min(a)
min(t)
data$Overall_Qual
train.x$Overall_Qual
train.x[Overall_Qual]
train.x
# Assume the Y value for the test data is stored in a two-column
# data frame named "test.y":
# col 1: PID
# col 2: Sale_Price
#code to test if a specific model satisfies the criteria. will output a vector showing which split fails
#it's more strict than the instructor's requirement. Because the instructor requires that for each split, as long as one model success, then you success.
#since each of us take over one model separately, we can only test one model. Hope our model successes consistently
#we could modified it on sunday to make it test for three models at the same time
#my poor english, ni neng kan dong ba?
testIDs = read.table("/Users/mahale/Documents/Punam/MSC/PSL/Project1/project1_testIDs.dat")
data <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/Ames_data.csv")
firstfivecriteria <- 0.125 #criteria applied to the first five split
secondfivecriteria <- 0.135 #criteria applied to the second five splits
ifsuc <- rep("SUC",10)
rate <- rep(0,10)
rate2 <- rep(0,10)
ifsuc2 <- rep("SUC",10)
rate3 <- rep(0,10)
ifsuc3 <- rep("SUC",10)
for (j in 1:10){
train <- data[-testIDs[,j], ]
test <- data[testIDs[,j], ]
test.y <- test[, c(1, 83)]
test <- test[, -83]
write.csv(train,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/train.csv",row.names=FALSE)
write.csv(test, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test.csv",row.names=FALSE)
write.csv(test.y, "/Users/mahale/Documents/Punam/MSC/PSL/Project1/test_y.csv",row.names=FALSE)
source('/Users/mahale/Documents/Punam/MSC/PSL/Project1/mymain.R')#your model here.make sure your model outputs "mysubmission.txt")
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission1.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc[j] <- "FAIL"
}
}
pred <- read.csv("/Users/mahale/Documents/Punam/MSC/PSL/Project1/mysubmission2.txt")
pred <- merge(pred, test.y, by="PID")
performance <- sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cat(performance)
rate2[j] <- performance
if (j <= 5){
if (performance >=firstfivecriteria){
ifsuc2[j] <- "FAIL"}
}else{
if (performance >=secondfivecriteria){
ifsuc2[j] <- "FAIL"
}
}
}
result <- data.frame(rate=rate,ifsuc=ifsuc,rate2=rate2,ifsuc2=ifsuc2,rate3=rate3,issuc3=ifsuc3)
write.csv(result,"/Users/mahale/Documents/Punam/MSC/PSL/Project1/finalresult.csv",row.names = FALSE)
shiny::runApp('Documents/Punam/MSC/PSL/Project4/BookRecommender copy')
library(shiny)
install.packages("shiny")
library(shiny)
install.packages("shiny")
setwd("~/Documents/Punam/MSC/PSL/Project4/BookRecommender copy")
?runApp
library(shiny)
devtools::session_info()
install.packages("shiny")
?library(shiny)
ui <- fluidPage(
)
server <- function(input, output, session) {
}
shinyApp(ui, server)
devtools::session_info()
install.packages("recommenderlab")
devtools::session_info()
library(recommenderlab)
?data.table
install.packages("data.table")
?data.table
library(data.table)
?data.table
install.packages("shinydashboard")
library(shinydashboard)
devtools::session_info()
?Rcpp::
?Rcpp
R.version()
R.version
installed.packages()
installed.packages()
"shiny" %in% rownames(installed.packages())
"shinydashboard" %in% rownames(installed.packages())
"shinydasshboard" %in% rownames(installed.packages())
"recommenderlab" %in% rownames(installed.packages())
"recommenderlab" %in% rownames(installed.packages())
"ShinyRatingInput" %in% rownames(installed.packages())
devtools::install_github("stefanwilhelm/ShinyRatingInput")
"Rcpp" %in% rownames(installed.packages())
remove.packages("Rcc", "RCC")
remove.packages("shiny", "shiny")
remove.packages("recommenderlab", "recommenderlab")
"recommenderlab" %in% rownames(installed.packages())
?uninstall.packages
?remove.packages
remove.packages(c("shiny"), c("shiny"))
library(utils)
remove.packages(c("shiny"), c("shiny"))
remove.packages()
install.packages(:shiny)
install.packages("shiny")
install.packages("dplyr")
library(dplyr)
library(data.table)
library(shinyjs)
library(ShinyRatingInput)
devtools::install_github("stefanwilhelm/ShinyRatingInput")
remove.packages(c("Rcpp"), c("Rcpp"))
installed.packages()
View(installed.packages())
library(ShinyRatingInput)
library(shiny)
R.version
